# -*- coding: utf-8 -*-
"""LSTM_V2_2Class.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-vunr3e0TEnPCwFzrKBCvj7IWbc6jMR3

# **Extract the Features of the Uniform Resource Locator(URL) Phase**

Uniform Resource Locator adalah alamat sumber daya standar di Internet, dan pintu masuk ke sebuah situs web. Pengacauan Uniform Resource Locator sangat umum terjadi pada phishing,
untuk memikat pengguna agar mengklik URL untuk mengunjungi situs web phishing mereka.

Untuk meningkatkan kemungkinan pengguna mengunjungi situs phishing, penyerang phishing sering menggunakan URL tipuan yang secara visual mirip dengan URL palsu. Format dari sebuah
URL standar adalah sebagai berikut:

**Protokol://hostname[:port]/path/[;parameter][?query]#fragment**

Cara umum untuk mengacaukan URL adalah dengan membuat URL phishing dengan memodifikasi dan mengganti bagian nama host dan bagian path berdasarkan URL target untuk membingungkan pengguna.

Sebagai contoh, penyerang menggunakan "www.amaz0n.com" sebagai situs web Amazon palsu (yang URL yang sebenarnya adalah "www.amazon.com"), atau menggunakan "www.interface-transport.com/ www.paypal.com/" sebagai situs web PayPal palsu (URL yang sebenarnya adalah "www.paypal.com") dan seterusnya.


Untuk mencapai tujuan ini, penyerang menggunakan berbagai teknik yang biasa digunakan untuk menyamarkan tautan phishing. Melalui teknik yang biasa digunakan penyerang, telah menemukan beberapa fitur yang dapat digunakan untuk menentukan apakah URL tersebut adalah tautan phishing:



1. Subdomain length
2. URL length Prefixes and suffixes in URLs
3. Punctuation counts
4. IP address
5. Port Number
6. URL Entropy
7. Other TLDs
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import os
import tensorflow as tf
import ipaddress
from urllib.parse import urlparse
import string
import math
from tensorflow.keras.optimizers import RMSprop
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

df = pd.read_csv('/content/drive/MyDrive/Capstone Project/Dataset/malicious_phishing_.csv')
df.head()

df["type"].value_counts()

# adding column feature url category
df["category"] = df["type"].replace({
    'legitimate':1,
    'phishing':0,
});

def url_length(url):
    # Remove common prefixes
    prefixes = ['http://', 'https://']
    for prefix in prefixes:
        if url.startswith(prefix):
            url = url[len(prefix):]

    # Remove common suffixes
    suffixes = ['.html', '.php', '.pdf']
    for suffix in suffixes:
        url = url.replace(suffix, '')

    # Return the length of the remaining URL
    return len(url)

df['URL Length'] = df['url'].apply(url_length)

def count_letters(url):
    return sum(char.isalpha() for char in url)

def count_digits(url):
    return sum(char.isdigit() for char in url)

def count_special_chars(url):
    special_chars = set(string.punctuation)
    return sum(char in special_chars for char in url)

# Menghitung jumlah huruf, angka, dan karakter khusus untuk setiap URL dalam DataFrame
df['Letters Count'] = df['url'].apply(count_letters)
df['Digits Count'] = df['url'].apply(count_digits)
df['Special Chars Count'] = df['url'].apply(count_special_chars)

class URLAnalyzer:
    def subdomains(self, url):
        subdomains = url.split('://')[-1].split('/')[0].split('.')
        return max(len(subdomains) - 2, 0)  # Calculate subdomains, excluding main domain and top-level domain

    def entropy(self, url):
        string = url.strip()
        prob = [float(string.count(c)) / len(string) for c in set(string)]
        entropy = -sum([(p * math.log2(p)) for p in prob if p != 0])  # Calculate entropy
        return entropy

# Creating an instance of URLAnalyzer
analyzer = URLAnalyzer()

# Calculating subdomains and entropy for each URL in the DataFrame
df['SubDomain'] = df['url'].apply(analyzer.subdomains)
df['Entropy'] = df['url'].apply(analyzer.entropy)

def have_ip_address(url):
    try:
        parsed_url = urlparse(url)
        if parsed_url.hostname:
            ip = ipaddress.ip_address(parsed_url.hostname)
            return isinstance(ip, (ipaddress.IPv4Address, ipaddress.IPv6Address))
    except ValueError:
        pass  # Invalid hostname or IP address

    return 0  # Return False if no valid IP address found

# Menghitung keberadaan alamat IP untuk setiap URL dalam DataFrame
df['Have IP Address'] = df['url'].apply(have_ip_address)

def check_port(url):
    # Mencari posisi ":" dalam URL yang menandakan adanya nomor port
    port_start = url.find(':')

    if port_start != -1:
        # Jika ":" ditemukan, kita mencari nomor port setelahnya
        port_str = url[port_start + 1:].split('/')[0]

        try:
            # Mengubah nomor port menjadi integer
            port = int(port_str)

            # Memeriksa apakah nomor port ada dalam daftar port HTTP yang dikenal
            if port in {21, 70, 80, 443, 1080, 8080}:
                return 1  # Return 1 if port is found in the known HTTP ports
            else:
                return 0  # Return 0 if port is not found in the known HTTP ports

        except ValueError:
            return 0  # Return 0 if there's an issue converting the port to an integer
    else:
        return 0  # Return 0 if no port is found in the URL

df['port']  = df['url'].apply(lambda x: check_port(x))

df.shape

df.columns

df.head()

df.to_csv('LSTM_DATA.csv', index=False)

"""# **Split Dataset Phase**"""

data = df.drop(columns=['type', 'url'])

data.to_csv('LSTM_DATA_2.csv', index=False)

data.columns

data.head()

x = data.drop(columns=['category'])
y = data['category']

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)

x_train_array = x_train.values.astype(np.float32)
x_test_array = x_test.values.astype(np.float32)

# Mengonversi y_train, y_test ke array NumPy
y_train_array = y_train.values.astype(np.float32)
y_test_array = y_test.values.astype(np.float32)

print(x_test_array)

print(f"X_train Shape : {x_train.shape}")
print(f"Y_train Shape : {y_train.shape}")
print(f"X_test  Shape : {x_test.shape}")
print(f"Y_test  Shape : {y_test.shape}")

"""# **Train Model**"""

# Model Definition with LSTM
model_lstm = tf.keras.Sequential([
        tf.keras.layers.Reshape((8, 1), input_shape=(8, )),
        tf.keras.layers.LSTM(64, return_sequences=True),
        tf.keras.layers.LSTM(64),
        tf.keras.layers.Dense(6, activation='relu'),
        tf.keras.layers.Dense(1, activation='sigmoid')
])

# Set the training parameters
model_lstm.compile(loss='binary_crossentropy', optimizer=RMSprop(learning_rate=1e-3) ,metrics=['accuracy'])

# Print the model summary
model_lstm.summary()

# Train the model
history_lstm = model_lstm.fit(x_train_array, y_train_array, epochs=50, verbose=2,  batch_size = 128, validation_data=(x_test_array, y_test_array))

# Plot Utility
def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.xlabel("Epochs")

  plt.ylabel(string)
  plt.legend([string, 'val_'+string])
  plt.show()

# Plot the accuracy and loss history
plot_graphs(history_lstm, 'accuracy')
plot_graphs(history_lstm, 'loss')

# Misalkan 'model_lstm' adalah model yang ingin Anda simpan
model_lstm.save('my_model.h5')

import tensorflow as tf

# Parameters
gru_dim = 64
dense_dim = 6

# Model Definition with GRU
model_gru = tf.keras.Sequential([
    tf.keras.layers.Reshape((8, 1), input_shape=(8, None)),
    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(gru_dim)),
    tf.keras.layers.Dense(dense_dim, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# Set the training parameters
model_gru.compile(loss='binary_crossentropy', optimizer=RMSprop(learning_rate=0.0001) ,metrics=['accuracy'])
# Print the model summary
model_gru.summary()

NUM_EPOCHS = 50
BATCH_SIZE = 128

# Train the model
history_gru = model_gru.fit(x_train_array, y_train_array, epochs=50, batch_size=BATCH_SIZE,  validation_data=(x_test_array, y_test_array))

import matplotlib.pyplot as plt

# Plot Utility
def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.legend([string, 'val_'+string])
  plt.show()

# Plot the accuracy and loss history
plot_graphs(history_gru, 'accuracy')
plot_graphs(history_gru, 'loss')

# Parameters
filters = 128
kernel_size = 5
dense_dim = 6

# Model Definition with Conv1D
model_conv = tf.keras.Sequential([
    tf.keras.layers.Reshape((8, 1), input_shape=(8, None)),
    tf.keras.layers.Conv1D(filters, kernel_size, activation='relu'),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(dense_dim, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# Set the training parameters
model_conv.compile(loss='binary_crossentropy', optimizer=RMSprop(learning_rate=1e-4) ,metrics=['accuracy'])

# Print the model summary
model_conv.summary()

NUM_EPOCHS = 50
BATCH_SIZE = 128

# Train the model
history_conv = model_conv.fit(x_train_array, y_train_array, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,  validation_data=(x_test_array, y_test_array))

import matplotlib.pyplot as plt

# Plot Utility
def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.legend([string, 'val_'+string])
  plt.show()

# Plot the accuracy and loss history
plot_graphs(history_conv, 'accuracy')
plot_graphs(history_conv, 'loss')

"""# **Model Evaluation Performance**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score
from sklearn.preprocessing import scale

y_pred = model_lstm.predict(x_test_array)
#
threshold = 0.6
y_pred_binary = [1 if pred >= threshold else 0 for pred in y_pred]

y_pred

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score



# Assuming y_true and y_pred are your true and predicted labels for multiclass classification
accuracy = accuracy_score(y_test_array, y_pred_binary)
precision = precision_score(y_test_array, y_pred_binary, average='weighted')
recall = recall_score(y_test_array, y_pred_binary, average='weighted')
f1 = f1_score(y_test_array, y_pred_binary, average='weighted')

# Display the metrics
print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")

from sklearn.metrics import mean_squared_error, mean_avarange_error,

mse = mean_squared_error(y_test_array, y_pred)
print("Mean Squared Error:", mse)

# The code below is to save your model as a .h5 file.
# It will be saved automatically in your Submission folder.
if __name__ == '__main__':
    # DO NOT CHANGE THIS CODE
    model = LSTM_V1_4Class()
    model.save("LSTM_V1_4Class.h5")

!pip install tensorflowjs as tfjs